var documenterSearchIndex = {"docs":
[{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Contents","page":"Reference","title":"Contents","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Index","page":"Reference","title":"Index","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [RegularizedProblems]","category":"page"},{"location":"reference/#RegularizedProblems.RegularizedNLPModel","page":"Reference","title":"RegularizedProblems.RegularizedNLPModel","text":"rmodel = RegularizedNLPModel(model, regularizer)\nrmodel = RegularizedNLSModel(model, regularizer)\n\nAn aggregate type to represent a regularized optimization model, .i.e., of the form\n\nminimize f(x) + h(x),\n\nwhere f is smooth (and is usually assumed to have Lipschitz-continuous gradient), and h is lower semi-continuous (and may have to be prox-bounded).\n\nThe regularized model is made of\n\nmodel <: AbstractNLPModel: the smooth part of the model, for example a FirstOrderModel\nh: the nonsmooth part of the model; typically a regularizer defined in ProximalOperators.jl\nselected: the subset of variables to which the regularizer h should be applied (default: all).\n\nThis aggregate type can be used to call solvers with a single object representing the model, but is especially useful for use with SolverBenchmark.jl, which expects problems to be defined by a single object.\n\n\n\n\n\n","category":"type"},{"location":"reference/#RegularizedProblems.MIT_matrix_completion_model-Tuple{}","page":"Reference","title":"RegularizedProblems.MIT_matrix_completion_model","text":"model, nls_model, sol = MIT_matrix_completion_model()\n\nA special case of matrix completion problem in which the exact image is a noisy MIT logo.\n\nSee the documentation of random_matrix_completion_model() for more information.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RegularizedProblems.bpdn_model-Tuple","page":"Reference","title":"RegularizedProblems.bpdn_model","text":"model, nls_model, sol = bpdn_model(args...; kwargs...)\nmodel, nls_model, sol = bpdn_model(compound = 1, args...; kwargs...)\n\nReturn an instance of an NLPModel and an instance of an NLSModel representing the same basis-pursuit denoise problem, i.e., the under-determined linear least-squares objective\n\n½ ‖Ax - b‖₂²,\n\nwhere A has orthonormal rows and b = A * x̄ + ϵ, x̄ is sparse and ϵ is a noise vector following a normal distribution with mean zero and standard deviation σ.\n\nArguments\n\nm :: Int: the number of rows of A\nn :: Int: the number of columns of A (with n ≥ m)\nk :: Int: the number of nonzero elements in x̄\nnoise :: Float64: noise standard deviation σ (default: 0.01).\n\nThe second form calls the first form with arguments\n\nm = 200 * compound\nn = 512 * compound\nk =  10 * compound\n\nKeyword arguments\n\nbounds :: Bool: whether or not to include nonnegativity bounds in the model (default: false).\n\nReturn Value\n\nAn instance of an NLPModel and of an NLSModel that represent the same basis-pursuit denoise problem, and the exact solution x̄.\n\nIf bounds == true, the positive part of x̄ is returned.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RegularizedProblems.group_lasso_model-Tuple","page":"Reference","title":"RegularizedProblems.group_lasso_model","text":"model, nls_model, sol = group_lasso_model(; kwargs...)\n\nReturn an instance of an NLPModel and NLSModel representing the group-lasso problem, i.e., the under-determined linear least-squares objective\n\n½ ‖Ax - b‖₂²,\n\nwhere A has orthonormal rows and b = A * x̄ + ϵ, x̄ is sparse and ϵ is a noise vector following a normal distribution with mean zero and standard deviation σ. Note that with this format, all groups have a the same number of elements and the number of groups divides evenly into the total number of elements.\n\nKeyword Arguments\n\nm :: Int: the number of rows of A (default: 200)\nn :: Int: the number of columns of A, with n ≥ m (default: 512)\ng :: Int: the number of groups (default: 16)\nag :: Int: the number of active groups (default: 5)\nnoise :: Float64: noise amount (default: 0.01)\ncompound :: Int: multiplier for m, n, g, and ag (default: 1).\n\nReturn Value\n\nAn instance of an NLPModel that represents the group-lasso problem. An instance of an NLSModel that represents the group-lasso problem. Also returns true x, number of groups g, group-index denoting which groups are active, and a Matrix where rows are group indices of x.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RegularizedProblems.nnmf_model","page":"Reference","title":"RegularizedProblems.nnmf_model","text":"model, nls_model, Av, selected = nnmf_model(m = 100, n = 50, k = 10, T = Float64)\n\nReturn an instance of an NLPModel and an NLSModel representing the non-negative matrix factorization objective\n\nf(W, H) = ½ ‖A - WH‖₂²,\n\nwhere A ∈ Rᵐˣⁿ has non-negative entries and can be separeted into k clusters, Av = A[:]. The vector of indices selected = k*m+1: k*(m+n) is used to indicate the components of W ∈ Rᵐˣᵏ and H ∈ Rᵏˣⁿ to apply  the regularizer to (so that the regularizer only applies to entries of H).\n\nArguments\n\nm :: Int: the number of rows of A\nn :: Int: the number of columns of A (with n ≥ m)\nk :: Int: the number of clusters\n\n\n\n\n\n","category":"function"},{"location":"reference/#RegularizedProblems.random_matrix_completion_model-Tuple{}","page":"Reference","title":"RegularizedProblems.random_matrix_completion_model","text":"model, nls_model, sol = random_matrix_completion_model(; kwargs...)\n\nReturn an instance of an NLPModel and an instance of an NLSModel representing the same matrix completion problem, i.e., the square linear least-squares objective\n\n½ ‖P(X - A)‖²\n\nin the Frobenius norm, where X is the unknown image represented as an m x n matrix, A is a fixed image, and the operator P only retains a certain subset of pixels of X and A.\n\nKeyword Arguments\n\nm :: Int: the number of rows of X and A (default: 100)\nn :: Int: the number of columns of X and A (default: 100)\nr :: Int: the desired rank of A (default: 5)\nsr :: AbstractFloat: a threshold between 0 and 1 used to determine the set of pixels\n\nretained by the operator P (default: 0.8)\n\nva :: AbstractFloat: the variance of a first Gaussian perturbation to be applied to A (default: 1.0e-4)\nvb :: AbstractFloat: the variance of a second Gaussian perturbation to be applied to A (default: 1.0e-2)\nc :: AbstractFloat: the coefficient of the convex combination of the two Gaussian perturbations (default: 0.2).\n\nReturn Value\n\nAn instance of an NLPModel and of an NLSModel that represent the same matrix completion problem, and the exact solution.\n\n\n\n\n\n","category":"method"},{"location":"#RegularizedProblems","page":"Home","title":"RegularizedProblems","text":"","category":"section"},{"location":"#Synopsis","page":"Home","title":"Synopsis","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package provides sameple problems suitable for developing and testing first and second-order methods for regularized optimization, i.e., they have the general form","category":"page"},{"location":"","page":"Home","title":"Home","text":"min_x in mathbbR^n  f(x) + h(x)","category":"page"},{"location":"","page":"Home","title":"Home","text":"where f mathbbR^n to mathbbR has Lipschitz-continuous gradient and h mathbbR^n to mathbbR cup infty is lower semi-continuous and proper. The smooth term f describes the objective to minimize while the role of the regularizer h is to select a solution with desirable properties: minimum norm, sparsity below a certain level, maximum sparsity, etc.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Models for f are instances of NLPModels and often represent nonlinear least-squares residuals, i.e., f(x) = tfrac12 F(x)_2^2 where F mathbbR^n to mathbbR^m.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The regularizer h should be obtained from ProximalOperators.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The final regularized problem is intended to be solved by way of solver for nonsmooth regularized optimization such as those in RegularizedOptimization.jl.","category":"page"},{"location":"#Problems-implemented","page":"Home","title":"Problems implemented","text":"","category":"section"},{"location":"#Basis-pursuit-denoise","page":"Home","title":"Basis-pursuit denoise","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Calling model = bpdn_model() returns a model representing the smooth underdetermined linear least-squares residual","category":"page"},{"location":"","page":"Home","title":"Home","text":"f(x) = tfrac12 Ax - b_2^2","category":"page"},{"location":"","page":"Home","title":"Home","text":"where A has orthonormal rows. The right-hand side is generated as b = A x_star + varepsilon where x_star is a sparse vector, varepsilon sim mathcalN(0 sigma) and sigma in (0 1) is a fixed noise level.","category":"page"},{"location":"","page":"Home","title":"Home","text":"When solving the basis-pursuit denoise problem, the goal is to recover x approx x_star. In particular, x should have the same sparsity pattern as x_star. That is typically accomplished by choosing a regularizer of the form","category":"page"},{"location":"","page":"Home","title":"Home","text":"h(x) = lambda x_1 for a well-chosen lambda  0;\nh(x) = x_0;\nh(x) = chi(x k mathbbB_0) for k approx x_star_0;","category":"page"},{"location":"","page":"Home","title":"Home","text":"where chi(x k mathbbB_0) is the indicator of the ell_0-pseudonorm ball of radius k.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Calling model = bpdn_nls_model() returns the same problem modeled explicitly as a least-squares problem.","category":"page"},{"location":"#Fitzhugh-Nagumo-data-fitting-problem","page":"Home","title":"Fitzhugh-Nagumo data-fitting problem","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If ADNLPModels and DifferentialEquations have been imported, model = fh_model() returns a model representing the over-determined nonlinear least-squares residual","category":"page"},{"location":"","page":"Home","title":"Home","text":"f(x) = tfrac12 F(x)_2^2","category":"page"},{"location":"","page":"Home","title":"Home","text":"where F mathbbR^5 to mathbbR^202 represents the residual between a simulation of the Fitzhugh-Nagumo system with parameters x and a simulation of the Van der Pol oscillator with preset, but unknown, parameters x_star.","category":"page"},{"location":"","page":"Home","title":"Home","text":"A feature of the Fitzhugh-Nagumo model is that it reduces to the Van der Pol oscillator when certain parameters are set to zero. Thus here again, the objective is to recover a sparse solution to the data-fitting problem. Hence, typical regularizers are the same as those used for the basis-pursuit denoise problem.","category":"page"}]
}
